{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "hBi7lFITLh7q",
      "metadata": {
        "id": "hBi7lFITLh7q"
      },
      "source": [
        "# Forward PINN (2D Acoustic) — **(using c=1, consistent ICs; hard IC/BC; dataset-driven)**\n",
        "\n",
        "This notebook trains a forward Physics-Informed Neural Network with:\n",
        "- constant wavespeed, $c \\equiv 1$,\n",
        "- **two IC snapshots** from the same $\\phi^*$ at $t=0$ and $t=\\tau$ (here $\\tau=0.05$), **enforced as hard constraints**,\n",
        "- Neumann **top boundary** $\\partial{\\phi}/\\partial z=0$ at $z=0$ which $\\phi^*$ satisfies are **hard-enforced**,\n",
        "- **limited dataset-based training samples** drawn from the analytical $\\phi^*$ and $s(x,z,t)$ for supervised consistency.\n",
        "\n",
        "**Domain:** $x,z\\in[0,1]$, training window $t\\in[0,0.5]$.  \n",
        "**Goal:** the PDE residual and data loss decrease together; IC/BC are satisfied by construction; and the snapshot error remains small (relative $L^2$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5XvYiH3mLh7w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XvYiH3mLh7w",
        "outputId": "f984f124-7cdb-4b40-a51e-5b00f47b42eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126 | CUDA available: True\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import numpy as np, matplotlib.pyplot as plt, math, time\n",
        "from dataclasses import dataclass\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "import os\n",
        "from typing import Tuple\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yuSW0V8l3g-R",
      "metadata": {
        "id": "yuSW0V8l3g-R"
      },
      "outputs": [],
      "source": [
        "# Import the data from google drive\n",
        "def load_component(\n",
        "    base_dir: str,\n",
        "    component: str,                # 'Z' or 'X'\n",
        "    t_s: float,                    # end time to keep (same meaning as your code)\n",
        "    t_st: float,                   # start time to keep\n",
        "    subsample: int = 100,          # like l_f in your code\n",
        "    device: torch.device | str = \"cpu\",\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, list[str]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      t_sub: (l_sub, 1) torch float32\n",
        "      S:     (n_seis * l_sub, 1) torch float32 stacked like your code\n",
        "      files: list of file names used (sorted)\n",
        "    \"\"\"\n",
        "    files = sorted([f for f in os.listdir(base_dir) if len(f) >= 6 and f[-6] == component])\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No {component}-component files found in {base_dir}\")\n",
        "    arrays = [np.loadtxt(os.path.join(base_dir, f)) for f in files]\n",
        "    # Specfem time shift to start at 0, same as: -arr[0,0] + arr[:,0]\n",
        "    t_spec = arrays[0][:, 0] - arrays[0][0, 0]\n",
        "    # Keep only [t_st, t_s]\n",
        "    mask = (t_spec <= t_s) & (t_spec >= t_st)\n",
        "    if not np.any(mask):\n",
        "        raise ValueError(\"No samples in the requested time window; check t_st and t_s.\")\n",
        "    first = np.argmax(mask)                       # first True\n",
        "    last  = len(mask) - 1 - np.argmax(mask[::-1]) # last True\n",
        "    index = np.arange(first, last + 1, subsample)\n",
        "    t_sub = t_spec[index].reshape(-1, 1)\n",
        "    # Stack station traces vertically, column 1 is amplitude (same as your [:,1])\n",
        "    S = np.concatenate([a[index, 1:2] for a in arrays], axis=0)\n",
        "    # Convert to torch\n",
        "    t_sub = torch.from_numpy(t_sub.astype(np.float32)).to(device)\n",
        "    S     = torch.from_numpy(S.astype(np.float32)).to(device)\n",
        "    return t_sub, S, files\n",
        "t_s = 0.75\n",
        "t_st = 0\n",
        "t_sub_Z, Sz, _ = load_component(\"/content/drive/MyDrive/seismograms\", \"Z\", t_s=t_s, t_st=t_st, subsample=100)\n",
        "t_sub_X, Sx, _ = load_component(\"/content/drive/MyDrive/seismograms\", \"X\", t_s=t_s, t_st=t_st, subsample=100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "zKlkjTQH3sSY",
      "metadata": {
        "id": "zKlkjTQH3sSY"
      },
      "outputs": [],
      "source": [
        "def build_X_S(t_sub: torch.Tensor,\n",
        "              z0_s: float, zl_s: float, n_seis: int,\n",
        "              ax: float, Lx: float = 1.0, Lz: float = 1.0,\n",
        "              device: str | torch.device = \"cpu\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Replicates your TF layout:\n",
        "      For i=0..n_seis-1:\n",
        "        x = ax/Lx (fixed), z = (z0_s - i*d_s)/Lz, t = t_sub\n",
        "    Returns: X_S of shape (n_seis*l_sub, 3)\n",
        "    \"\"\"\n",
        "    t_sub = t_sub.to(device).reshape(-1, 1)         # (l_sub, 1)\n",
        "    l_sub = t_sub.shape[0]\n",
        "    d_s = abs(zl_s - z0_s) / max(n_seis - 1, 1)\n",
        "    rows = []\n",
        "    for i in range(n_seis):\n",
        "        x_col = torch.full((l_sub, 1), ax / Lx, dtype=t_sub.dtype, device=device)\n",
        "        z_col = torch.full((l_sub, 1), (z0_s - i * d_s) / Lz, dtype=t_sub.dtype, device=device)\n",
        "        rows.append(torch.cat([x_col, z_col, t_sub], dim=1))\n",
        "    return torch.vstack(rows)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "pxl3hsndLh7z",
      "metadata": {
        "id": "pxl3hsndLh7z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Domain:\n",
        "    ax: float = 1.0     # x-extent\n",
        "    az: float = 1.0     # z-extent\n",
        "    t_max: float = 0.5  # training time window\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    batch_size: int = 20000\n",
        "    adam_lr: float = 1e-3\n",
        "    adam_epochs: int = 1000\n",
        "    use_lbfgs: bool = True\n",
        "    lbfgs_max_iter: int = 250\n",
        "    seed: int = 1234\n",
        "@dataclass\n",
        "class Weights:\n",
        "    w_pde: float = 1.0\n",
        "    w_ic1: float = 1.0\n",
        "    w_ic2: float = 1.0\n",
        "    w_seis: float = 1.0\n",
        "    w_bc_top_neu: float = 1.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "W05XbGR9Lh74",
      "metadata": {
        "id": "W05XbGR9Lh74"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim=3, out_dim=1, hidden=128, layers=6, act=nn.Tanh):\n",
        "        super().__init__()\n",
        "        net, last = [], in_dim\n",
        "        for _ in range(layers):\n",
        "            net += [nn.Linear(last, hidden), act()]; last = hidden\n",
        "        net += [nn.Linear(last, out_dim)]\n",
        "        self.net = nn.Sequential(*net)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2aa8f60e",
      "metadata": {
        "id": "2aa8f60e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ForwardPINN:\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network for the 2D acoustic wave equation (c = 1).\n",
        "    - Optional source function can be provided via `source_fn`.\n",
        "    - Optional hard constraints can be applied outside this class via an output transform on the model.\n",
        "    - Optional IC targets can be supplied via `set_ic_targets`; otherwise IC losses default to zero.\n",
        "    \"\"\"\n",
        "    def __init__(self, dom: Domain, model: nn.Module, source_fn=None, device_override=None):\n",
        "        self.dom = dom\n",
        "        self.model = model.to(device if device_override is None else device_override)\n",
        "        self.device = device if device_override is None else device_override\n",
        "        self.source_fn = source_fn  # callable: (x,z,t)->tensor or None\n",
        "        self._X_S = None\n",
        "        self._Sx = None\n",
        "        self._Sz = None\n",
        "\n",
        "        # Optional IC targets (soft constraints). If unset, they are ignored.\n",
        "        self._ic_grid = None  # tuple (Xg, Zg)\n",
        "        self._ic_times = None # list of t tensors\n",
        "        self._ic_targets = None # list of (Ux_tgt, Uz_tgt)\n",
        "\n",
        "    @staticmethod\n",
        "    def grad(y, x):\n",
        "        return torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y),\n",
        "                                   create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    def pde_residual(self, pts):\n",
        "        \"\"\"\n",
        "        Residual for phi_tt - phi_xx - phi_zz = s(x,z,t), with c = 1.\n",
        "        Returns (eq, phi_z) where phi_z is used for a Neumann top boundary condition if desired.\n",
        "        \"\"\"\n",
        "        pts = pts.requires_grad_(True)\n",
        "        phi = self.model(pts)\n",
        "        x, z, t = pts[:,0:1], pts[:,1:2], pts[:,2:3]\n",
        "        d = self.grad(phi, pts)\n",
        "        phi_x, phi_z, phi_t = d[:,0:1], d[:,1:2], d[:,2:3]\n",
        "        phi_xx = self.grad(phi_x, pts)[:,0:1]\n",
        "        phi_zz = self.grad(phi_z, pts)[:,1:2]\n",
        "        phi_tt = self.grad(phi_t, pts)[:,2:3]\n",
        "\n",
        "        if self.source_fn is None:\n",
        "            src = torch.zeros_like(phi)\n",
        "        else:\n",
        "            # Ensure source is evaluated on the correct device and shape\n",
        "            src = self.source_fn(x, z, t)\n",
        "            if src.shape != phi.shape:\n",
        "                raise ValueError(\"source_fn must return a tensor with shape matching phi. \"\n",
        "                                 f\"Got {src.shape}, expected {phi.shape}.\")\n",
        "\n",
        "        eq = phi_tt - phi_xx - phi_zz - src   # c = 1\n",
        "        return eq, phi_z\n",
        "\n",
        "    # Samplers\n",
        "    def sample_pde(self, n):\n",
        "        x = torch.rand(n,1, device=self.device) * self.dom.ax\n",
        "        z = torch.rand(n,1, device=self.device) * self.dom.az\n",
        "        t = torch.rand(n,1, device=self.device) * self.dom.t_max\n",
        "        return torch.cat([x,z,t], dim=1)\n",
        "\n",
        "    def sample_top(self, n):\n",
        "        x = torch.rand(n,1, device=self.device) * self.dom.ax\n",
        "        z = torch.zeros(n,1, device=self.device)  # top boundary at z=0\n",
        "        t = torch.rand(n,1, device=self.device) * self.dom.t_max\n",
        "        return torch.cat([x,z,t], dim=1)\n",
        "\n",
        "    def sample_ic_grid(self, n):\n",
        "        xs = torch.linspace(0, self.dom.ax, n, device=self.device)\n",
        "        zs = torch.linspace(0, self.dom.az, n, device=self.device)\n",
        "        X, Z = torch.meshgrid(xs, zs, indexing='ij')\n",
        "        return X.reshape(-1,1), Z.reshape(-1,1)\n",
        "\n",
        "    def set_seismo_data(self, X_S: torch.Tensor, Sx: torch.Tensor, Sz: torch.Tensor):\n",
        "        self._X_S = X_S.to(self.device).reshape(-1, 3)\n",
        "        self._Sx   = Sx.to(self.device).reshape(-1, 1)\n",
        "        self._Sz   = Sz.to(self.device).reshape(-1, 1)\n",
        "\n",
        "    def set_ic_targets(self, Xg, Zg, times, targets):\n",
        "        \"\"\"\n",
        "        Provide soft initial-condition targets:\n",
        "        - Xg, Zg: flattened coordinate grids (N,1) tensors on device\n",
        "        - times: list of t tensors, each (N,1) on device\n",
        "        - targets: list of (Ux_tgt, Uz_tgt), each (N,1) pair\n",
        "        \"\"\"\n",
        "        self._ic_grid = (Xg, Zg)\n",
        "        self._ic_times = times\n",
        "        self._ic_targets = targets\n",
        "\n",
        "    # displacement-only seismogram loss\n",
        "    def seismo_loss(self):\n",
        "        if self._X_S is None:\n",
        "            return torch.tensor(0.0, device=self.device)\n",
        "        ux, uz = self.predict_displacement_components(self._X_S)\n",
        "        return torch.mean((ux - self._Sx)**2) + torch.mean((uz - self._Sz)**2)\n",
        "\n",
        "    def predict_displacement_components(self, pts):\n",
        "        # Assuming displacement components are the spatial gradients of phi\n",
        "        pts = pts.requires_grad_(True)\n",
        "        phi = self.model(pts)\n",
        "        d = self.grad(phi, pts)\n",
        "        ux, uz = d[:, 0:1], d[:, 1:2]\n",
        "        return ux, uz\n",
        "\n",
        "    def _ic_losses(self):\n",
        "        if self._ic_grid is None or self._ic_times is None or self._ic_targets is None:\n",
        "            return []\n",
        "        Xg, Zg = self._ic_grid\n",
        "        losses = []\n",
        "        for tvec, (Ux_tgt, Uz_tgt) in zip(self._ic_times, self._ic_targets):\n",
        "            pts = torch.cat([Xg, Zg, tvec], dim=1).requires_grad_(True)\n",
        "            phi = self.model(pts)\n",
        "            d = self.grad(phi, pts)\n",
        "            ux, uz = d[:,0:1], d[:,1:2]\n",
        "            losses.append(torch.mean((ux - Ux_tgt)**2) + torch.mean((uz - Uz_tgt)**2))\n",
        "        return losses\n",
        "\n",
        "    def train(self, cfg: TrainCfg, w: Weights, X_S: torch.Tensor=None, Sx: torch.Tensor=None, Sz: torch.Tensor=None):\n",
        "        torch.manual_seed(cfg.seed)\n",
        "        opt = torch.optim.Adam(self.model.parameters(), lr=cfg.adam_lr)\n",
        "\n",
        "        # Optional: seismogram supervision\n",
        "        if X_S is not None and Sx is not None and Sz is not None:\n",
        "            self.set_seismo_data(X_S, Sx, Sz)\n",
        "\n",
        "        # Adam phase\n",
        "        for epoch in range(1, cfg.adam_epochs+1):\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # PDE residual\n",
        "            eq, _ = self.pde_residual(self.sample_pde(cfg.batch_size))\n",
        "            L_pde = torch.mean(eq**2)\n",
        "\n",
        "            # Top Neumann boundary (dphi/dz = 0)\n",
        "            _, phi_z_top = self.pde_residual(self.sample_top(max(1, cfg.batch_size//4)))\n",
        "            L_bc = torch.mean(phi_z_top**2)\n",
        "\n",
        "            # IC losses (if provided)\n",
        "            ic_losses = self._ic_losses()\n",
        "            L_ic = sum(ic_losses) if ic_losses else torch.tensor(0.0, device=self.device)\n",
        "\n",
        "            # Seismogram loss (if provided)\n",
        "            L_seis = self.seismo_loss()\n",
        "\n",
        "            loss = w.w_pde*L_pde + w.w_bc_top_neu*L_bc + w.w_ic1*L_ic + w.w_seis*L_seis\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            if epoch % 200 == 0 or epoch == 1:\n",
        "                print(f\"[Adam {epoch:4d}] total={loss.item():.3e} | pde={L_pde.item():.3e} | bc={L_bc.item():.3e} | ic={L_ic.item():.3e} | seis={L_seis.item():.3e}\")\n",
        "\n",
        "        # L-BFGS refine (optional)\n",
        "        if getattr(cfg, \"use_lbfgs\", False):\n",
        "            print(\"\\\\nSwitching to L-BFGS...\")\n",
        "            lbfgs = torch.optim.LBFGS(self.model.parameters(),\n",
        "                                      max_iter=cfg.lbfgs_max_iter,\n",
        "                                      line_search_fn=\"strong_wolfe\")\n",
        "            def closure():\n",
        "                lbfgs.zero_grad()\n",
        "                eq, _ = self.pde_residual(self.sample_pde(cfg.batch_size))\n",
        "                L_pde = torch.mean(eq**2)\n",
        "                _, phi_z_top = self.pde_residual(self.sample_top(max(1, cfg.batch_size//4)))\n",
        "                L_bc = torch.mean(phi_z_top**2)\n",
        "                ic_losses = self._ic_losses()\n",
        "                L_ic = sum(ic_losses) if ic_losses else torch.tensor(0.0, device=self.device)\n",
        "                loss = w.w_pde*L_pde + w.w_bc_top_neu*L_bc + w.w_ic1*L_ic + w.w_seis*self.seismo_loss()\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            lbfgs.step(closure)\n",
        "            print(\"L-BFGS done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "YRcLwP1DLh76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRcLwP1DLh76",
        "outputId": "7dbeb553-39a2-4392-d053-7112ceddb7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Adam    1] total=1.519e-02 | pde=2.637e-04 | bc=5.981e-03 | ic=0.000e+00 | seis=9.205e-03\n",
            "[Adam  200] total=4.577e-07 | pde=3.227e-06 | bc=5.437e-09 | ic=0.000e+00 | seis=4.200e-07\n",
            "[Adam  400] total=1.503e-07 | pde=3.840e-06 | bc=1.818e-09 | ic=0.000e+00 | seis=1.101e-07\n",
            "[Adam  600] total=5.481e-08 | pde=2.104e-06 | bc=4.385e-09 | ic=0.000e+00 | seis=2.939e-08\n",
            "[Adam  800] total=2.775e-08 | pde=1.563e-06 | bc=3.900e-09 | ic=0.000e+00 | seis=8.223e-09\n",
            "[Adam 1000] total=1.845e-08 | pde=1.281e-06 | bc=2.402e-09 | ic=0.000e+00 | seis=3.235e-09\n",
            "[Adam 1200] total=1.397e-08 | pde=1.076e-06 | bc=1.318e-09 | ic=0.000e+00 | seis=1.889e-09\n",
            "[Adam 1400] total=1.131e-08 | pde=9.052e-07 | bc=8.102e-10 | ic=0.000e+00 | seis=1.449e-09\n",
            "[Adam 1600] total=9.756e-09 | pde=7.795e-07 | bc=6.597e-10 | ic=0.000e+00 | seis=1.301e-09\n",
            "[Adam 1800] total=8.353e-09 | pde=6.485e-07 | bc=6.285e-10 | ic=0.000e+00 | seis=1.239e-09\n",
            "[Adam 2000] total=7.164e-09 | pde=5.334e-07 | bc=6.230e-10 | ic=0.000e+00 | seis=1.207e-09\n",
            "[Adam 2200] total=6.269e-09 | pde=4.459e-07 | bc=6.314e-10 | ic=0.000e+00 | seis=1.179e-09\n",
            "[Adam 2400] total=5.503e-09 | pde=3.725e-07 | bc=6.246e-10 | ic=0.000e+00 | seis=1.153e-09\n",
            "[Adam 2600] total=4.850e-09 | pde=3.103e-07 | bc=6.148e-10 | ic=0.000e+00 | seis=1.132e-09\n",
            "[Adam 2800] total=4.366e-09 | pde=2.638e-07 | bc=6.176e-10 | ic=0.000e+00 | seis=1.111e-09\n",
            "[Adam 3000] total=4.037e-09 | pde=2.310e-07 | bc=6.352e-10 | ic=0.000e+00 | seis=1.091e-09\n",
            "\\nSwitching to L-BFGS...\n",
            "L-BFGS done.\n"
          ]
        }
      ],
      "source": [
        "dom = Domain()\n",
        "model = MLP(hidden=128, layers=6)\n",
        "solver = ForwardPINN(dom, model)\n",
        "cfg = TrainCfg(adam_epochs=3000, lbfgs_max_iter=500, batch_size=32000)\n",
        "wts = Weights(w_pde=0.01, w_ic1=1.0, w_ic2=1.0, w_bc_top_neu=1.0,w_seis=1.0)\n",
        "X_S = build_X_S(\n",
        "    t_sub=t_sub_Z,         # use the same t grid you used for Sz/Sx\n",
        "    z0_s=0.01, zl_s=1,\n",
        "    n_seis=20,\n",
        "    ax=dom.ax, Lx=3, Lz=3,\n",
        "    device=device\n",
        ")\n",
        "solver.train(cfg, wts, X_S, Sx, Sz)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "87clGGW9nZYt",
      "metadata": {
        "id": "87clGGW9nZYt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# --- Hard-constraint utilities (no MMS) ---\n",
        "L   = 1.0   # domain length in x (set to your domain)\n",
        "H   = 1.0   # domain height in z (set to your domain)\n",
        "def Bx(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Zero at x=0 and x=L; O(1) inside. Prevents correction from changing Dirichlet BCs in x.\"\"\"\n",
        "    xl = x / L\n",
        "    return xl * (1.0 - xl)\n",
        "def Bz(z: torch.Tensor, both_ends: bool = False) -> torch.Tensor:\n",
        "    \"\"\"Neumann at z=0 (and optionally at z=H).\"\"\"\n",
        "    zh = z / H\n",
        "    if not both_ends:\n",
        "        return zh ** 2\n",
        "    return (zh ** 2) * ((1.0 - zh) ** 2)\n",
        "class ConstrainedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    φ(x,z,t) = D_x(x,z,t) + Bx(x) * Bz(z) * uθ(x,z,t)\n",
        "      - D_x(x,z,t) = ((L-x)/L)*d1(z,t) + (x/L)*d2(z,t) enforces Dirichlet at x=0 and x=L.\n",
        "    Default here uses d1=d2=0 (homogeneous Dirichlet). Modify constants below if needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_net: nn.Module, both_ends_neumann: bool = False,\n",
        "                 d1_value: float = 0.0, d2_value: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.base = base_net\n",
        "        self.both_ends_neumann = both_ends_neumann\n",
        "        self.register_buffer(\"d1_const\", torch.tensor(float(d1_value)))\n",
        "        self.register_buffer(\"d2_const\", torch.tensor(float(d2_value)))\n",
        "    def forward(self, xzt: torch.Tensor) -> torch.Tensor:\n",
        "        x = xzt[:, 0:1]\n",
        "        z = xzt[:, 1:2]\n",
        "        # t = xzt[:, 2:3]  # time left unconstrained\n",
        "        # Hard Dirichlet in x\n",
        "        d1 = self.d1_const.expand_as(x)\n",
        "        d2 = self.d2_const.expand_as(x)\n",
        "        D  = ((L - x) / L) * d1 + (x / L) * d2\n",
        "        # Interior correction with optional Neumann in z\n",
        "        u  = self.base(xzt)\n",
        "        return D + Bx(x) * Bz(z, both_ends=self.both_ends_neumann) * u\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "arQrTnJgfDsz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arQrTnJgfDsz",
        "outputId": "aa006ef2-155a-4ca4-852a-48454ff7f106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Adam    1] total=3.228e-04 | pde=1.051e-02 | bc=0.000e+00 | ic=0.000e+00 | seis=2.177e-04\n",
            "[Adam  200] total=1.800e-09 | pde=1.149e-07 | bc=0.000e+00 | ic=0.000e+00 | seis=6.512e-10\n",
            "[Adam  400] total=9.716e-10 | pde=4.106e-08 | bc=0.000e+00 | ic=0.000e+00 | seis=5.609e-10\n",
            "[Adam  600] total=6.948e-10 | pde=1.548e-08 | bc=0.000e+00 | ic=0.000e+00 | seis=5.400e-10\n",
            "[Adam  800] total=6.064e-10 | pde=7.091e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.355e-10\n",
            "[Adam 1000] total=5.674e-10 | pde=3.354e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.339e-10\n",
            "[Adam 1200] total=5.549e-10 | pde=2.194e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.329e-10\n",
            "[Adam 1400] total=5.499e-10 | pde=1.742e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.325e-10\n",
            "[Adam 1600] total=5.480e-10 | pde=1.600e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.320e-10\n",
            "[Adam 1800] total=5.463e-10 | pde=1.468e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.317e-10\n",
            "[Adam 2000] total=5.445e-10 | pde=1.310e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.314e-10\n",
            "[Adam 2200] total=5.440e-10 | pde=1.289e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.311e-10\n",
            "[Adam 2400] total=5.424e-10 | pde=1.147e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.310e-10\n",
            "[Adam 2600] total=5.416e-10 | pde=1.085e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.308e-10\n",
            "[Adam 2800] total=5.407e-10 | pde=1.029e-09 | bc=0.000e+00 | ic=0.000e+00 | seis=5.304e-10\n",
            "[Adam 3000] total=5.401e-10 | pde=9.803e-10 | bc=0.000e+00 | ic=0.000e+00 | seis=5.303e-10\n",
            "\\nSwitching to L-BFGS...\n",
            "L-BFGS done.\n"
          ]
        }
      ],
      "source": [
        "# Replace the plain MLP with the constrained wrapper\n",
        "base_net = MLP(hidden=128, layers=6)\n",
        "model = ConstrainedModel(base_net).to(device)\n",
        "solver = ForwardPINN(dom, model)\n",
        "# With hard constraints, set IC/BC weights to 0 (no need to optimize them)\n",
        "cfg = TrainCfg(adam_epochs=3000, lbfgs_max_iter=500, batch_size=32000)  # keep your values if different\n",
        "wts = Weights(w_pde=0.01, w_ic1=0.0, w_ic2=0.0, w_bc_top_neu=0.0,w_seis=1.0)\n",
        "X_S = build_X_S(\n",
        "    t_sub=t_sub_Z,         # use the same t grid you used for Sz/Sx\n",
        "    z0_s=0.01, zl_s=1,\n",
        "    n_seis=20,\n",
        "    ax=dom.ax, Lx=3, Lz=3,\n",
        "    device=device\n",
        ")\n",
        "solver.train(cfg, wts, X_S, Sx, Sz)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}